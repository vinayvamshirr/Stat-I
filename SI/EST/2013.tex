\section*{2013}
\vspace{-.5cm}
\hrulefill \smallskip\\
\ques{1}{c}{10} Let $X_1$, $X_2$ be independent random variables following Poisson distribution with parameter $\lambda$. Show that $X_1 + X_2$ is sufficient for $\lambda$. Verify if $X_1 + 2X_2$ is also sufficient for $\lambda$.
\myline
\ques{2}{c}{15} Let $(X,Y)$ be jointly distributed with probability density function \[ \begin{aligned}f_{X,Y}(x,y,\theta) &= \exp[- (\frac{x}{\theta} + \theta y)], \enskip &&X> 0, Y>0 \\
&=0, &&\text{otherwise}
\end{aligned}\] Find Fisher information in a sample of $n$ pairs.
\myline
\ques{3}{a}{20} Using the Rao-Blackwell Theorem, derive the uniformly minimum variance unbiased estimator (UMVUE) of an estimable parametric function based on  a complete sufficient statistic.\\ Obtain the UMVUE of $P[X =r], \enskip r=0,1,\dotsc,m$ where $X_1,X_2,\dotsc,X_n$ are independently identically distributed (i.i.d.) $Bin(m,p)$.
\myline
\ques{3}{c}{15} Let $X$ be distributed as $Bin(n,p)$ and $\pi(p) = 1 \enskip(0 <p<1)$ be the prior probability density of $p$. Then obtain the Bayes estimator (under squared error loss) and Bayes risk.
