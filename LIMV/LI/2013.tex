\section*{2013}
\vspace{-.5cm}
\hrulefill \smallskip\\
\ques{5}{a}{10} Consider the following linear model
\[ \begin{aligned} y_1 &= \theta_1 + \theta_2 + \theta_3 + \theta_4 + \epsilon_1 \\
y_2 &= \theta_1 + \theta_3 - \theta_2 -\theta_4 + \epsilon_2 \\
y_3 &= \theta_1 + \theta_2 - \theta_3 -\theta_4 + \epsilon_3 \\
y_4 &= \theta_1 + \theta_4 - \theta_2 -\theta_3 + \epsilon_4 \\
\end{aligned}\] where $\theta_1,\theta_2,\theta_3$ and $\theta_4$ are parameters, $\epsilon_j \sim (0,\sigma^2)$, $j = 1,\ldots,4$. $\epsilon_j$'s are all independently distributed. Derive normal equations and obtain the BLUE for $\theta_1 + \theta_2 + \theta_3 + \theta_4$. Also find the variance of the BLUE.
\myline
\ques{5}{b}{10} Consider a  model $y_{ij} = \mu + \alpha_i + \epsilon_{ij}$, $i = 1,\ldots,k$, $j=1,\ldots,n$ , $\epsilon_{ij}$'s are i.i.d. N$(0, \sigma^2)$. Obtain least squares solutions of $\mu$ and $\alpha_i$. When is a linear parametric function of $\alpha_i$'s (only) estimable? Derive a test statistic for testing $\displaystyle H_0 : \sum_{i =1}^{n}C_i\alpha_i = 0$.